[
  %% SASL config
  {sasl, [
    {sasl_error_logger, {file, "log/sasl-error.log"}},
    {errlog_type, error},
    {error_logger_mf_dir, "log/sasl"},      % Log directory
    {error_logger_mf_maxbytes, 10485760},   % 10 MB max file size
    {error_logger_mf_maxfiles, 5}           % 5 files max
  ]},
  {lager, [
    %% What handlers to install with what arguments
    %% The defaults for the logfiles are to rotate the files when
    %% they reach 10Mb or at midnight, whichever comes first, and keep
    %% the last 31 rotations.
    {handlers, [
          {lager_console_backend, debug},
         {lager_file_backend, [{file, "log/error.log"}, {level, '=error'}, {size, 104857600}, {date, "$D0"}, {count, 5}]},     %% 100Mb
         {lager_file_backend, [{file, "log/warning.log"}, {level, '=warning'}, {size, 104857600}, {date, "$D0"}, {count, 5}]}, %% 100Mb
         {lager_file_backend, [{file, "log/info.log"}, {level, '=info'}, {size, 1073741824}, {date, "$D0"}, {count, 10}]},     %% 1Gb
         {lager_file_backend, [{file, "log/debug.log"}, {level, '=debug'}, {size, 104857600}, {date, "$D0"}, {count, 5}]}      %% 100Mb
    ]},
    %% Whether to write a crash log, and where.
    %% Commented/omitted/undefined means no crash logger.
    {crash_log, "log/crash.log"},
    %% Maximum size in bytes of events in the crash log - defaults to 65536
    {crash_log_msg_size, 65536},
    %% Maximum size of the crash log in bytes, before its rotated, set
    %% to 0 to disable rotation - default is 0
    {crash_log_size, 10485760},
    %% What time to rotate the crash log - default is no time
    %% rotation.
    {crash_log_date, "$D0"},
    %% Number of rotated crash logs to keep, 0 means keep only the
    %% current one - default is 0
    {crash_log_count, 5},
    %% Whether to redirect error_logger messages into lager - defaults to true
    {error_logger_redirect, true},
    {async_threshold, 200},
    {async_threshold_window, 50}
  ]},
  %% {conveyor,  %conveyor for working with privat system
  %%   [
  %%     {box, false}
  %%   ]
  %% },
  {conveyor,  %conveyor for working with privat system
  [
    {prod, false},
    {test, false},
    { {{ eprolog_namespace }}, false}
    ]
  },
  {sysmon,  %conveyor for reporting statistics
    [
      { {{ eprolog_namespace }}, false}
    ]
  },
  {weight,  %conveyor for working fact's weight
    [
      { {{ eprolog_namespace }}, false}
    ]
  },
  {get_weight,  %conveyor for get fact's weight
    [
      { {{ eprolog_namespace }}, false}
    ]
  },
  {ep_data_pool, % pool for working with new database mongo 3.0
    [
      { {{ eprolog_namespace }},
        [
          [
            {host, "{{ mongodb_host }}"},
            {port, "{{ mongodb_port }}"},
            {deployment, single}
          ],
          [
            {register, ep_data_pool},
            {pool_size, {{ ep_mongodb_pool_size }}},
            {max_overflow, 0},
            {localThresholdMS, 1000},
            {connectTimeoutMS, 20000},
            {socketTimeoutMS, 100},
            {serverSelectionTimeoutMS, 20000},
            {waitQueueTimeoutMS, 1000},
            {heartbeatFrequencyMS, 10000},
            {minHeartbeatFrequencyMS, 1000},
            {rp_mode, primaryPreferred},
            {rp_tags, []}
          ],
          [
            {database, <<"{{ mongodb_deepmeo_db_name }}">>},
            {login, <<"{{ mongodb_deepmemo_user }}">>},
            {auth_source, <<"{{ mongodb_deepmeo_db_name }}">>},
            {password, <<"{{ mongodb_deepmemo_password }}">>},
            {w_mode, safe}
          ]
        ]
      }
    ]
  },

{ep_cache_pool,  % pool for cache workers with reddis
    [
      { {{ eprolog_namespace }},
        [
          {eredis_cluster,
            [
              {init_nodes,
                [
                   {"{{ redis_host }}", {{redis_port}}}
                ]
              },
              {pool_size, 5},
              {pool_max_overflow, 0}
            ]
          }
        ]
      }
    ]
  },
{ep_hash_key,
    [
      { {{ eprolog_namespace }}, "one_hash"}
    ]
  },
  {httpc_profiles,
    [
      {name, "eprolog_#"},
      {size, 25}
    ]
  },
  {zabbix_sender,
    [
      {status, off},
      {zabbix_host, "zabbix_host"},
      {zabbix_port, 10051}
    ]
  },
  {antispam, [{status, false}]},
  {bins,
    [
      {status, off},
      {ftp_host, "ftp_host"},
      {ftp_user, "login"},
      {ftp_pass, "password"},
      {file_name, "rootfilename_"}
    ]
  },
  {ep_escode_container,
    [
      { {{ eprolog_namespace }},
        [
          {separator, <<"/eprolog/">>},
          {es_files_dir, <<"/eprolog_files/">>}
        ]
      }
    ]
  },
  {ep_statistics_reporter,
    [
      {status, off}
    ]
  },
  {ep_statistics_reporter_to_zabbix,
    [
      {status, off}
    ]
  },
  {ep_statistics_reporter_to_conveyor,
    [
      {status, off}
    ]
  },
  {ep_weight_worker,
    [
      {status, off}
    ]
  },
  {ep_isp_catalog,
    [
      {status, off},
      {ftp_host, "ftp_host"},
      {ftp_user, "user"},
      {ftp_pass, "pass"},
      {file_name, "filename.csv"}
    ]
  },
  {ep_terrorist_catalog,
    [
      {status, [{dj, off}, {lat, off}]},
      {host, "djrcfeed.dowjones.com"},
      {basic_auth, "Auth"},
      {working_nodes, [
        "eprolog@127.0.0.1"
      ]},
      {ftp_host, "host"},
      {ftp_user, "user"},
      {ftp_pass, "password"},
      {ftp_file_name, "filename.csv"}
    ]
  },
  {ep_batch_lager,
    [
      {status, off}
    ]
  },
  {ep_query_params_saver,
    [
      {status, off}
    ]
  },
  {cron_jobs,
    [
      {jobs,
        [
        ]
      }
    ]
  },
  {ep_write_to_log,
    [
      {more_then, 100} % in milliseconds
    ]
  },
  {conf_namespace,
    [
      {conf, {{ eprolog_namespace }} }
    ]
  },
{erlog_worker,
     [
       {prodd, false},
       {test, false},
       { {{ eprolog_namespace }},
         [
           {max_heap_size, 10000000}
         ]
       }
     ]
   },
  {reconnect,
     [
       {intensity, infinity}, % количество попыток переустановки коннектов, после которых приложение тушим
       {check_status_interval, 5000}, % временной интервал для проверок состояния коннектов в миллисекундах
       {time_for_reconnecting, infinity}, % период времени в миллисекундах, за который пытаемся переустановить коннекты и если за это время не успеваем, то тушим приложение
       {check_error_interval, 1000} % временной интервал для проверок в миллисекундах с момента первого обнаружения ошибки в коннектах
     ]
  }
].