[

    {conv_params, [
        {min_version, {{ worker_conv_params_min_version | default(2) }}} %% application should deny creating task (copy/rpc) if params are not valid.
                     %% We have in database conveyor table conveyors field version. If version in DB >= min_version in config we run validator else ignore.
    ]},

    {kernel, [
        {inet_dist_listen_min, {{ worker_cluster_port }}},
        {inet_dist_listen_max, {{ worker_cluster_port }}}
    ]},

    {corezoid_license_client, [
        {path_to_license, "/ebsmnt/certs/{{ license_file_name }}"}
    ]},

    {mw_metrics, [
      {is_enabled, {{ prometheus_metrics | default("false") }} },
      {subsystems, [erlprometheus]}
    ]},

    {erlprometheus, [
      {host, {0,0,0,0}},
      {port, 9100}
    ]},

    %% global_stats
    {corezoid_global_stats, [
        {disabled, true},
        {host, "gs.corezoid.com"},
        {port, 443},
        {send_stat_interval, 60} %% in seconds
    ]},

    {limits_client, [
        {redis, [
            [
                {host, "{{ redis_cache[0].host }}"},
                {port, {{ redis_cache[0].port }}},
                {database, 11},
                {password, "{{ redis_cache[0].password }}"},
                {start_size, 2},
                {min_size, 2},
                {max_size, 2}
            ]
        ]}
    ]},

    {env_var, [

        %% - psql pool -
        {psql, [
            {host, "{{ db_main.host }}"},
            {dbname, "conveyor"},
            {user, "{{ db_main.user }}"},
            {port, 5432},
            {password, "{{ db_main.pass }}"},
            {start_size, 3},
            {min_size, 3},
            {max_size, 30}
        ]}
    ]},

    {dns_cache, [
        {servers, [
{% for item in rmq_core %}
            [
                {name, {{ item.dns_cache_name }}},
                {dns, "{{ item.host }}"},
                {ttl, {{ dns_cache_ttl | default(60) }}}
            ],
{% endfor %}
            [
                {name, {{ rmq_http[0].dns_cache_name }}},
                {dns, "{{ rmq_http[0].host }}"},
                {ttl, {{ dns_cache_ttl | default(60) }}}
            ]
        ]}
    ]},

    %% for clustering components
    {corezoid_cluster, [
        {backend, redis}, %% maybe if future list will increase
        {redis, [
            {host, "{{ redis_cache[0].host }}"},
            {port, {{ redis_cache[0].port }}},
            {database, 10},
            {password, "{{ redis_cache_password | default("") }}"}
        ]}
    ]},

{% if db_call is defined and db_call %}
    {mw_db_call, [
        {enabled, true},
        {db_call_host, "{{ db_call_url }}"},
        {request_amqp, [
            {host, <<"{{ db_call_host_rmq }}">>},
            {port, {{ rmq[0].port }}},
            {username, <<"{{ rmq[0].user }}">>},
            {password, <<"{{ rmq[0].pass }}">>},
            {vhost, <<"{{ db_call_vhost }}">>},
            {exchange, <<"db-call">>},
            {is_durable_exchange, true},
            {is_auto_delete_exchange, false},
            {prefetch, 100},
            {is_auto_delete_queue, false},
            {is_durable_queue, true}
        ]},
        {response_amqp, [
            {host, <<"{{ db_call_host_rmq }}">>},
            {port, {{ rmq[0].port }}},
            {username, <<"{{ rmq[0].user }}">>},
            {password, <<"{{ rmq[0].pass }}">>},
            {vhost, <<"{{ db_call_vhost }}">>}
        ]},
        {response_queue, <<"mw_dunderdbcall_response_queue_">>}
    ]},
{% endif %}
{% if db_callv2 is defined and db_callv2 %}
    {mw_db_call, [
        {version, 2},
        {enabled, true},
        {request_amqp_v2, [
            {host, <<"{{ db_call_host_rmq }}">>},
            {port, {{ rmq[0].port }}},
            {username, <<"{{ rmq[0].user }}">>},
            {password, <<"{{ rmq[0].pass }}">>},
            {vhost, <<"{{ db_call_vhost }}">>},
            {exchange, <<"">>}
        ]}
    ]},
{% endif %}

    {ermql, [

        {publish_request, [
            {servers, [
                [
                    %%{dns_name, {{ rmq_http[0].dns_cache_name }}},
                    {host, "{{ rmq_http[0].host }}"},
                    {port, {{ rmq_http[0].port }}},
                    {username, <<"{{ rmq_http[0].user }}">>},
                    {password, <<"{{ rmq_http[0].pass }}">>},
                    {vhost, <<"{{ rmq_vhost }}">>}
                ]
{% if db_call is defined and db_call %}
                ,[
                    {host, "{{ db_call_host_rmq }}"},
                    {port, {{ rmq[0].port }}},
                    {username, <<"{{ rmq[0].user }}">>},
                    {password, <<"{{ rmq[0].pass }}">>},
                    {vhost, <<"{{ db_call_vhost }}">>},
                    {pools, [dbcall]}
                ]
{% endif %}
{% if db_callv2 is defined and db_callv2 %}
                ,[
                    {host, "{{ db_call_host_rmq }}"},
                    {port, {{ rmq[0].port }}},
                    {username, <<"{{ rmq[0].user }}">>},
                    {password, <<"{{ rmq[0].pass }}">>},
                    {vhost, <<"{{ db_call_vhost }}">>},
                    {pools, [dbcall]}
                ]
{% endif %}
            ]},
            {queues_count, 1},
            {min_size, 1},
            {max_size, 1},
            {start_size, 1}
        ]},

        {consumer_response, [
            {servers, [
                [
                    %%{dns_name, {{ rmq_http[0].dns_cache_name }}},
                    {host, "{{ rmq_http[0].host }}"},
                    {port, {{ rmq_http[0].port }}},
                    {username, <<"{{ rmq_http[0].user }}">>},
                    {password, <<"{{ rmq_http[0].pass }}">>},
                    {vhost, <<"{{ rmq_vhost }}">>}
                ]
{% if db_call is defined and db_call %}
                ,[
                    {host, "{{ db_call_host_rmq }}"},
                    {port, {{ rmq[0].port }}},
                    {username, <<"{{ rmq[0].user }}">>},
                    {password, <<"{{ rmq[0].pass }}">>},
                    {vhost, <<"{{ db_call_vhost }}">>},
                    {pools, [dbcall]}
                ]
{% endif %}
{% if db_callv2 is defined and db_callv2 %}
                ,[
                    {host, "{{ db_call_host_rmq }}"},
                    {port, {{ rmq[0].port }}},
                    {username, <<"{{ rmq[0].user }}">>},
                    {password, <<"{{ rmq[0].pass }}">>},
                    {vhost, <<"{{ db_call_vhost }}">>},
                    {pools, [dbcall]}
                ]
{% endif %}
            ]},
            {connections_per_queue, 1},
            {channels_per_connection, 1},
            {messages_prefetch_size_per_channel, 50}
        ]}
    ]},

    {corezoid_queues_gc, [
        {disabled, {{ worker_corezoid_queues_gc_disabled | default("false") }}},
        {host, "{{ rmq_http[0].host }}"},
        {port, 15672},
        {vhost, "{{ rmq_http[0].vhost }}"},
        {login, "conveyor_gc"},
        {password, "{{ rmq_http[0].pass }}"},
        {gc_queues_regexp, ["worker.ctrl", "to_worker_.*.cce.*","to_worker_.*.es_call.*","to_worker_.*.git_call.*","settings.worker"]}
    ]},
{% if enigma_is_enabled == "true" %}
    {enigma, [
        {is_enabled, {{ enigma_is_enabled }}},
        {private_key_id, "{{ enigma_private_key_id }}"},
        {key_manager_host, "{{ enigma_key_manager_host }}"},

        {client_cert, "{{ enigma_client_cert }}"},
        {client_key, "{{ enigma_client_key }}"},
        {ca_cert, "{{ enigma_ca_cert }}"},

        {rotors_pool, [
            {min_size, 5},
            {max_size, 5},
            {start_size, 5}
        ]}
    ]},
{% else %}
    {enigma, [
        {is_enabled, {{ enigma_is_enabled }}}
    ]},
{% endif %}
    { worker,
        [
            %% server id
            { worker_id, <<"worker-{{ worker.id | default(ansible_default_ipv4.address) }}">> },
            {prometheus_metrics, {{ prometheus_metrics | default("false") }} },

            {nodes, [
{% for item in worker_hosts %}
{% if worker_hosts.index(item) == worker_hosts|length - 1 %}
                '{{ item.host }}'
{% else %}
                '{{ item.host }}',
{% endif %}
{% endfor %}
            ]},

            {use_limits_from_server, {{ capi_use_limits_from_server | default("false") }}},

            %% for ENCRYPT/DECRYPT data
            {encrypt_decrypt, [
            ]},

            %% max task size for process conv
            {max_task_size_for_process_conv, {{ worker_max_task_size_for_process_conv }}},

            %% max task size for state diagramm conv
            {max_task_size_for_st_diagramm_conv, {{ worker_max_task_size_for_st_diagramm_conv }}},

            %% sequence of shard numbers
            %% if you want to distribute the workload
            %% between several workers
            %% you can switch any part of this numbers to
            %% another application
            { shards, {{ shards }} },

            %% shard's number
            %% it depends on the real shards number in DB
            %% or DB mappings in pgbouncer
            { shards_count, {{ shards_count }} },

            %% swtich on\off timer logic execution on start worker app
            %% if "false" timers won't work
            { timer_auto_init, {{ timer_auto_init | default("true") }} },

            {type_storage, persistent}, %% persistent | memory

            %% this param needs to dynamically create public callback url in constructions:
            %% conv[ID].node[Node].public_callback_corezoid
            %% conv[ID].node[Node].public_callback_mandrill
            %% node[Node].public_callback_corezoid
            %% node[Node].public_callback_corezoid
            %% It should be in binary format
            {public_callback_prefix, <<"{{ worker_public_callback_prefix }}">>},

            %% DB settings
            { pgsql,
                [
                    { host, "{{ db_main.host }}" },
                    { hosts, [
{% for item in db_shards %}
{% if db_shards.index(item) == db_shards|length - 1 %}
                        { {{ item.shards }}, "{{ item.host }}" }
{% else %}
                        { {{ item.shards }}, "{{ item.host }}" },
{% endif %}
{% endfor %}
                    ]},
                    { user, "{{ db_main.user }}" },
                    { dbname, "conveyor" },
                    { password, "{{ db_main.pass }}" },
                    { start_size, {{ pg_conn_pool_start | default(2) }} },
                    { min_size, {{ pg_conn_pool_min | default(0) }} },
                    { max_size, {{ pg_conn_pool_max | default(1000) }} }
                ]
            },

            %% PostgreSQL archive DB
            { pgsql_archive,
                [
                    { hosts, [
{% for item in db_archive_shards %}
{% if db_archive_shards.index(item) == db_archive_shards|length - 1 %}
                        { {{ item.shards }}, "{{ item.host }}" }
{% else %}
                        { {{ item.shards }}, "{{ item.host }}" },
{% endif %}
{% endfor %}
                    ]},
                    { user, "{{ db_main.user }}" },
                    { password, "{{ db_main.pass }}" },
                    { start_size, {{ pg_conn_pool_start | default(2) }} },
                    { min_size, {{ pg_conn_pool_min | default(0) }} },
                    { max_size, {{ pg_conn_pool_max | default(1000) }} }
                ]
            },

            %% PostgreSQL tasks_history DB
            { pgsql_task_history,
                [
                    {hosts, [
{% for item in db_archive_shards %}
{% if db_archive_shards.index(item) == db_archive_shards|length - 1 %}
                        { {{ item.shards }}, "{{ item.host }}" }
{% else %}
                        { {{ item.shards }}, "{{ item.host }}" },
{% endif %}
{% endfor %}
                    ]},
                    {user, "{{ db_main.user }}" },
                    {password, "{{ db_main.pass }}" },
                    {start_size, {{ pg_conn_pool_start | default(2) }} },
                    {min_size, {{ pg_conn_pool_min | default(0) }} },
                    {max_size, {{ pg_conn_pool_max | default(50) }} }
                ]
            },

            {write_data_to_history, {{ worker_write_data_to_history | default("false") }}}, %% true|false - write or not tasks data into table, default - false

            % postgresql settings for statistics db
            {pgsql_statistics, [
                {host, "{{ db_main.host }}"},
                {user, "{{ db_main.user }}"},
                {dbname, "conveyor_statistics"},
                {password, "{{ db_main.pass }}"},
                {start_size, 1},
                {min_size, 0},
                {max_size, 1}
            ]},

            {statistics, {{ statistics_enable | default("false") }}},    % true|false for enable or disable statistic workers by default is true

            %% for custom db-s
            { pgsql2, [] },

            %% redis sentinel quorum hosts
            {sentinels, []},

            %% persistent storage for different counters
            %% please don't forget about backup of this data
            { redis, [
{% for item in redis_counters %}
                [
                    {host, "{{ item.host }}"},
                    {port, {{ item.port }}},
                    {database,{{ item.db }}},
                    {password,"{{ item.password }}"},
                    {start_size, {{ redis_conn_pool_start | default(10) }}},
                    {min_size, {{ redis_conn_pool_min | default(10) }}},
                    {max_size, {{ redis_conn_pool_max | default(100) }}}
{% if redis_counters.index(item) == redis_counters|length - 1 %}
                ]
{% else %}
                ],
{% endif %}
{% endfor %}
            ]},

      %% persistent storage for api_sum counters
      %% please don't forget about backup of this data
      { redis_api_sum, [
{% for item in redis_api_sum %}
        [
          {host, "{{ item.host }}"},
          {port, {{ item.port }}},
          {database, {{ item.db }}},
          {password, "{{ item.password }}"},
          {start_size, {{ redis_api_sum_pool_start | default(5) }}},
          {min_size, {{ redis_api_sum_pool_min | default(5) }}},
          {max_size, {{ redis_api_sum_pool_max | default(50) }}}
{% if redis_api_sum.index(item) == redis_api_sum|length - 1 %}
        ]
{% else %}
        ],
{% endif %}
{% endfor %}
      ]},

      %% cache, nonpersistent storage, keys will expire after N hours
      { redis2, [
{% for item in redis_cache %}
        [
          {host, "{{ item.host }}"},
          {port, {{ item.port }}},
          {database, {{ item.db }}},
          {password, "{{ item.password }}"},
          {start_size, {{ redis2_conn_pool_start | default(50) }}},
          {min_size, {{ redis2_conn_pool_min | default(50) }}},
          {max_size, {{ redis2_conn_pool_max | default(200) }}}
{% if redis_cache.index(item) == redis_cache|length - 1 %}
        ]
{% else %}
        ],
{% endif %}
{% endfor %}
      ]},

      %% memory redis for timer calls
      {redis_timers, [
{% for item in redis_timers %}
        [
          {host, "{{ item.host }}"},
          {port, {{ item.port }}},
          {database, {{ item.db }}},
          {password, "{{ item.password }}"},
          {min_size, {{ redis_timers_conn_pool_min | default(10) }}},
          {max_size, {{ redis_timers_conn_pool_max | default(200) }}},
          {start_size, {{ redis_timers_conn_pool_start | default(10) }}}
{% if redis_timers.index(item) == redis_timers|length - 1 %}
        ]
{% else %}
        ],
{% endif %}
{% endfor %}
      ]},

      %% cache size in redis2
      {redis2_ttl, {{ redis2_ttl | default(7200) }}},

      %% publish tasks to queue
      %% for delivery them to storage
      {publish_unload_archive_task, [
        {servers, [[
          %{dns_name, main},
          {host, "{{ rmq_host }}"},
          {port, {{ rmq_port }}},
          {username, <<"{{ rmq_user }}">>},
          {password, <<"{{ rmq_user_pass }}">>},
          {vhost, <<"{{ rmq_vhost }}">>}
        ]]},
        {min_size, {{ pub_arc_pool_min }}},
        {max_size, {{ pub_arc_pool_max }}},
        {start_size, {{ pub_arc_pool_start }}}
      ]},

      {consumer_unload_archive_task, [
        {servers, [[
          %%{dns_name, main},
          {host, '{{ rmq_host }}'},
          {port, {{ rmq_port }}},
          {username, <<"{{ rmq_user }}">>},
          {password, <<"{{ rmq_user_pass }}">>},
          {vhost, <<"{{ rmq_vhost }}">>}
        ]]},
        {connections_per_queue, {{ cons_arc_conn_per_queue}}},
        {channels_per_connection, {{ cons_arc_channels_per_connection }}},
        {messages_prefetch_size_per_channel, {{ cons_arc_msg_prefetch_size }}},
        {workers, {{ cons_arc_workers }}}
        %% connections_per_queue * channels_per_connection * messages_prefetch_size_per_channel % workers = batch size per worker
      ]},

      %% deprecated, producer of copy_task requests
      {publish_copy_task_request, [
        {servers, [[
          %{dns_name, main},
          {host, '{{ rmq_host }}'},
          {port, {{ rmq_port }}},
          {username, <<"{{ rmq_user }}">>},
          {password, <<"{{ rmq_user_pass }}">>},
          {vhost, <<"{{ rmq_vhost }}">>}
        ]]},
        {queues_count, {{ pub_copy_task_queues_count }}},
        {min_size, {{ pub_copy_task_pool_min }}},
        {max_size, {{ pub_copy_task_pool_max }}},
        {start_size, {{ pub_copy_task_pool_start }}}
        %%{is_sharded, true} %% deprecated
      ]},

      %% deprecated, consumer for responses of copy_tasks
      {consumer_copy_task_response, [
        {servers, [[
          %{dns_name, main},
          {host, '{{ rmq_host }}'},
          {port, {{ rmq_port }}},
          {username, <<"{{ rmq_user }}">>},
          {password, <<"{{ rmq_user_pass }}">>},
          {vhost, <<"{{ rmq_vhost }}">>}
        ]]},
        {connections_per_queue, {{ cons_copy_task_connections_per_queue }}},
        {channels_per_connection, {{ cons_copy_task_channels_per_connection }}},
        {messages_prefetch_size_per_channel, {{ cons_copy_task_msg_prefetch_size }}}
      ]},


      %% producer of http reqeuests, API logic
      {publish_http_request, [
        {servers, [[
          %{dns_name, name5},
          {host, '{{ rmq_http[0].host }}'},
          {port, {{ rmq_port }}},
          {username, <<"{{ rmq_user }}">>},
          {password, <<"{{ rmq_user_pass }}">>},
          {vhost, <<"{{ rmq_vhost }}">>}
        ]]},
        {queues_count, {{ pub_http_queues_count }}},
        {min_size, {{ pub_http_pool_min }}},
        {max_size, {{ pub_http_pool_max }}},
        {start_size, {{ pub_http_pool_start }}}
      ]},

      %% consumer for http responses, API logic
      {consumer_http_response, [
        {servers, [[
          %{dns_name, name5},
          {host, '{{ rmq_http[0].host }}'},
          {port, {{ rmq_http[0].port }}},
          {username, <<"{{ rmq_http[0].user }}">>},
          {password, <<"{{ rmq_http[0].pass }}">>},
          {vhost, <<"{{ rmq_vhost }}">>}
        ]]},
        {connections_per_queue, {{ cons_http_connections_per_queue }}},
        {channels_per_connection, {{ cons_http_channels_per_connection }}},
        {messages_prefetch_size_per_channel, {{ cons_http_msg_prefetch_size }}}
      ]},

      %% producer of rpc requests, RPC logic
      {publish_rpc_request, [
        {servers, [[
          %{dns_name, main},
          {host, '{{ rmq_host }}'},
          {port, {{ rmq_port }}},
          {username, <<"{{ rmq_user }}">>},
          {password, <<"{{ rmq_user_pass }}">>},
          {vhost, <<"{{ rmq_vhost }}">>}
        ]]},
        {queues_count, {{ pub_rpc_queues_count }}},
        {min_size, {{ pub_rpc_pool_min }}},
        {max_size, {{ pub_rpc_pool_max }}},
        {start_size, {{ pub_rpc_pool_start }}},
        {is_sharded, true}
      ]},

      %% consumer for rpc responses, RPC logic
      {consumer_rpc_response, [
        {servers, [[
          %{dns_name, main},
          {host, '{{ rmq_host }}'},
          {port, {{ rmq_port }}},
          {username, <<"{{ rmq_user }}">>},
          {password, <<"{{ rmq_user_pass }}">>},
          {vhost, <<"{{ rmq_vhost }}">>}
        ]]},
        {connections_per_queue, {{ cons_rpc_connections_per_queue }}},
        {channels_per_connection, {{ cons_rpc_channels_per_connection }}},
        {messages_prefetch_size_per_channel, {{ cons_rpc_msg_prefetch_size }}}
      ]},

      %% producer of usercode requests, CODE logic
      {publish_cce_request, [
        {servers, [[
          %{dns_name, main},
          {host, '{{ rmq_host }}'},
          {port, {{ rmq_port }}},
          {username, <<"{{ rmq_user }}">>},
          {password, <<"{{ rmq_user_pass }}">>},
          {vhost, <<"{{ rmq_vhost }}">>}
        ]]},
        {queues_count, {{ pub_cce_queues_count }}},
        {min_size, {{ pub_cce_pool_min }}},
        {max_size, {{ pub_cce_pool_max }}},
        {start_size, {{ pub_cce_pool_start }}}
      ]},

      %% consumer for usercode responses, CODE logic
      {consumer_cce_response, [
        {servers, [[
          {host, '{{ rmq_host }}'},
          {port, {{ rmq_port }}},
          {username, <<"{{ rmq_user }}">>},
          {password, <<"{{ rmq_user_pass }}">>},
          {vhost, <<"{{ rmq_vhost }}">>}
        ]]},
        {connections_per_queue, {{ cons_cce_connections_per_queue }}},
        {channels_per_connection, {{ cons_cce_channels_per_connection }}},
        {messages_prefetch_size_per_channel, {{ cons_cce_msg_prefetch_size }}}
      ]},


       {% if git_call is defined and git_call %}
        {git_call_v2, [
          %% Optional. Boolean. Default: false.
          %% Enable\Disable GitCallV2 feature.
          %% Config sections
          %%   publish_git_call_v2_request
          %%   consumer_git_call_v2_response
          %%   consumer_git_call_v2_status
          %%   publish_dunder_git_call_v2_request
          %%   consumer_dunder_git_call_v2_request
          %% must be configured if enabled, otherwise it is safe to completely remove them.
          %% If disabled, worker packet returns {error, git_calL_disabled} on all related commands.
          {enabled, true},

          %% Optional. Number. Milliseconds. Default: ?RUN_TASK_TIME_TO_WAIT_CONSUMER 100.
          %% The maximum time the code is allowed to work.
          {run_task_time_to_wait_consumer, 10000},

          %% Optional. Boolean. Default: ?USE_ENIGMA true.
          %% Controls whether the commutation with Gitcall has to be encrypted by Enigma or not.
          {use_enigma, false},

          %% Optional. Boolean. Default: false.
          %% Set to true to get debug output from Gitcall module(s).
          {debug, false}
        ]},


        %% Required if git_call_v2.enabled true, otherwise could be omitted.
        %% logic git call v2
        %% producer
        {publish_git_call_v2_request, [
          {servers, [[
           {host, "{{ rmq[0].host }}"},
           {port, {{ rmq[0].port }}},
           {username, <<"{{ rmq[0].user }}">>},
           {password, <<"{{ rmq[0].pass }}">>},
           {vhost, <<"{{ git_call_vhost }}">>}
          ]]},

          %% Optional. BinaryString. Default: ?GITCALL_EXCHANGE <<"gitcall-v2">>.
          %% The exchange is used to communicate with Gitcall service.
          %% All calls to Gitcall go through the exchange.
          %%{exchange, <<"gitcall-v2">>},

          %% Optional. BinaryString. Default: ?BUILD_SERVICE_QUEUE <<"gitcall-v2-buildservice-">>.
          %% The queue is used to build code an pack it ready-to-run units, most possibly docker images
          %% The count option controls how many prefixed queues to publish to.
          %%
          %%{buildservice_queue, <<"gitcall-v2-buildservice-">>},
          %%{buildservice_queues_count, 1},

          %% Optional. BinaryString. Default: ?BUILD_ARCHIVE_QUEUE <<"gitcall-v2-buildarchive-">>.
          %% The queue is used to retrieve build achieve.
          %% The archive contains build status which could be "done", "error", "internal_error", "not_found", "in_progress"
          %% The count option controls how many prefixed queues to publish to.
          %%
          %%{buildarchive_queue, <<"gitcall-v2-buildarchive-">>},
          %%{buildarchive_queues_count, 1},

          %% Optional. BinaryString. Default: ?DEPLOY_SERVICE_QUEUE <<"gitcall-v2-deployservice-">>.
          %% The queue is used to deploy services to production runtime, most possibly k8s
          %% The count option controls how many prefixed queues to publish to.
          %%
          %%{deployservice_queue, <<"gitcall-v2-deployservice-">>},
          %%{deployservice_queues_count, 1},

          %% Optional. BinaryString. Default: ?GET_SERVICE_QUEUE <<"gitcall-v2-getservice-">>.
          %% The queue is used to get service details: source, built and deployed versions.
          %% The count option controls how many prefixed queues to publish to.
          %%
          %%{getservice_queue, <<"gitcall-v2-getservice-">>},
          %%{getservice_queues_count, 1},

          %% Optional. BinaryString. Default: ?REMOVE_SERVICE_QUEUE <<"gitcall-v2-removeservice-">>.
          %% The queue is used to remove service from production runtime and Gitcall database.
          %% The count option controls how many prefixed queues to publish to.
          %%
          %%{removeservice_queue, <<"gitcall-v2-getservice-">>},
          %%{removeservice_queues_count, 1},

          %% Optional. BinaryString. Default: ?RUN_SERVICE_QUEUE <<"gitcall-v2-runservice-">>.
          %% The queue is used to ask Gitcall to start (if not) service in production runtime. Often used together with stopservice
          %% The count option controls how many prefixed queues to publish to.
          %%
          %%{runservice_queue, <<"gitcall-v2-runservice-">>},
          %%{runservice_queues_count, 1},

          %% Optional. BinaryString. Default: ?SAVE_SERVICE_QUEUE <<"gitcall-v2-saveservice-">>.
          %% The queue is used to save temporary changes. Used by Corezoid sandbox
          %% The count option controls how many prefixed queues to publish to.
          %%
          %%{saveservice_queue, <<"gitcall-v2-saveservice-">>},
          %%{saveservice_queues_count, 1},

          %% Optional. BinaryString. Default: ?STOP_SERVICE_QUEUE <<"gitcall-v2-stopservice-">>.
          %% The queue is used to stop service in production runtime when idle. Could be run later with runservice call
          %% The count option controls how many prefixed queues to publish to.
          %%
          %%{stopservice_queue, <<"gitcall-v2-stopservice-">>},
          %%{stopservice_queues_count, 1},

          %% Optional. BinaryString. Default: ?VALIDATE_SERVICE_QUEUE <<"gitcall-v2-validateservice-">>.
          %% The queue is used to validate the source code.
          %% If API returns done the source is already built and ready for deploy.
          %% The count option controls how many prefixed queues to publish to.
          %%
          %%{validateservice_queue, <<"gitcall-v2-validateservice-">>},
          %%{validateservice_queues_count, 1},

          {min_size, 2},
          {max_size, 2},
          {start_size, 2}
        ]},

        %% Required if git_call_v2.enabled true, otherwise could be omitted.
        %% consumer processes replies on worker requests
        {consumer_git_call_v2_response, [
          {servers, [[
           {host, "{{ rmq[0].host }}"},
           {port, {{ rmq[0].port }}},
           {username, <<"{{ rmq[0].user }}">>},
           {password, <<"{{ rmq[0].pass }}">>},
           {vhost, <<"{{ git_call_vhost }}">>}
          ]]},
          {connections_per_queue, 1},
          {channels_per_connection, 1},
          {messages_prefetch_size_per_channel, 50}
        ]},

        %% Required if git_call_v2.enabled true, otherwise could be omitted.
        %% consumer processes status updates from gitcall
        {consumer_git_call_v2_status, [
          {servers, [[
           {host, "{{ rmq[0].host }}"},
           {port, {{ rmq[0].port }}},
           {username, <<"{{ rmq[0].user }}">>},
           {password, <<"{{ rmq[0].pass }}">>},
           {vhost, <<"{{ git_call_vhost }}">>}
          ]]},
          {connections_per_queue, 1},
          {channels_per_connection, 1},
          {messages_prefetch_size_per_channel, 50}
        ]},

        %% Required if git_call_v2.enabled true, otherwise could be omitted.
        %% logic dunder git call v2
        %% producer
        {publish_dunder_git_call_v2_request, [
          {servers, [[
           {host, "{{ rmq[0].host }}"},
           {port, {{ rmq[0].port }}},
           {username, <<"{{ rmq[0].user }}">>},
           {password, <<"{{ rmq[0].pass }}">>},
           {vhost, <<"{{ git_call_dunder_vhost }}">>}
          ]]},
          {min_size, 5},
          {max_size, 5},
          {start_size, 5}
        ]},

        %% Required if git_call_v2.enabled true, otherwise could be omitted.
        %% consumer
        {consumer_dunder_git_call_v2_response, [
          {servers, [[
           {host, "{{ rmq[0].host }}"},
           {port, {{ rmq[0].port }}},
           {username, <<"{{ rmq[0].user }}">>},
           {password, <<"{{ rmq[0].pass }}">>},
           {vhost, <<"{{ git_call_dunder_vhost }}">>}
          ]]},
          {connections_per_queue, 1},
          {channels_per_connection, 1},
          {messages_prefetch_size_per_channel, 50}
        ]},
        {% endif %}
        %% logic db call
        %% producer
        {publish_db_call_request, [
            {servers, [[
                {host, "{{ rmq[0].host }}"},
                {port, {{ rmq[0].port }}},
                {username, <<"{{ rmq[0].user }}">>},
                {password, <<"{{ rmq[0].pass }}">>},
                {vhost, <<"{{ rmq_vhost }}">>}
            ]]},
            {min_size, {{ pub_db_call_pool_min }}},
            {max_size, {{ pub_db_call_pool_max }}},
            {start_size, {{ pub_db_call_pool_start }}}
        ]},


        %% consumer
        {consumer_db_call_response, [
            {servers, [
                [
                    {host, "{{ rmq[0].host }}"},
                    {port, {{ rmq[0].port }}},
                    {username, <<"{{ rmq[0].user }}">>},
                    {password, <<"{{ rmq[0].pass }}">>},
                    {vhost, <<"{{ db_call_vhost }}">>}
                ]
            ]},
            %{queues_count, 10},
            {connections_per_queue, 1},
            {channels_per_connection, 2},
            {messages_prefetch_size_per_channel, 50}
        ]},

        %% logic deep_memo es call
        %% producer
        {publish_es_call_request, [
            {servers, [
                [
                    %{dns_name, main},
                    {host, '{{ rmq_host }}'},
                    {port, {{ rmq_port }}},
                    {username, <<"{{ rmq_user }}">>},
                    {password, <<"{{ rmq_user_pass }}">>},
                    {vhost, <<"{{ rmq_vhost }}">>}
                ]
            ]},
            {min_size, {{ pub_es_call_pool_min }}},
            {max_size, {{ pub_es_call_pool_max }}},
            {start_size, {{ pub_es_call_pool_start }}}
        ]},

        %% consumer
        {consumer_es_call_response, [
            {servers, [
                [
                    %{dns_name, main},
                    {host, '{{ rmq_host }}'},
                    {port, {{ rmq_port }}},
                    {username, <<"{{ rmq_user }}">>},
                    {password, <<"{{ rmq_user_pass }}">>},
                    {vhost, <<"{{ rmq_vhost }}">>}
                ]
            ]},
            %{queues_count, 10},
            {connections_per_queue, 1},
            {channels_per_connection, 1},
            {messages_prefetch_size_per_channel, 50}
        ]},

      %% producer of get_task requests, QUEUE logic
      {publish_get_task_request, [
        {servers, [[
          %{dns_name, main},
          {host, '{{ rmq_host }}'},
          {port, {{ rmq_port }}},
          {username, <<"{{ rmq_user }}">>},
          {password, <<"{{ rmq_user_pass }}">>},
          {vhost, <<"{{ rmq_vhost }}">>}
        ]]},
        {queues_count, {{ pub_get_task_queues_count }}},
        {min_size, {{ pub_get_task_pool_min }}},
        {max_size, {{ pub_get_task_pool_max }}},
        {start_size, {{ pub_get_task_pool_start }}}
      ]},

      %% consumer for get_task requests, QUEUE logic
      {consumer_get_task_response, [
        {servers, [[
          %{dns_name, main},
          {host, '{{ rmq_host }}'},
          {port, {{ rmq_port }}},
          {username, <<"{{ rmq_user }}">>},
          {password, <<"{{ rmq_user_pass }}">>},
          {vhost, <<"{{ rmq_vhost }}">>}
        ]]},
        {connections_per_queue, {{ cons_get_task_connections_per_queue }}},
        {channels_per_connection, {{ cons_get_task_channels_per_connection }}},
        {messages_prefetch_size_per_channel, {{ cons_get_task_msg_prefetch_size }}}
      ]},

      %% producer of modify requests, QUEUE logic
      {publish_modify_request, [
        {servers, [[
          %{dns_name, main},
          {host, '{{ rmq_host }}'},
          {port, {{ rmq_port }}},
          {username, <<"{{ rmq_user }}">>},
          {password, <<"{{ rmq_user_pass }}">>},
          {vhost, <<"{{ rmq_vhost }}">>}
        ]]},
        {min_size, {{ pub_modify_pool_min }}},
        {max_size, {{ pub_modify_pool_max }}},
        {start_size, {{ pub_modify_pool_start }}}
      ]},

      %% producer of settings
      %% this messages inform app-s about changes in a structure of processes,
      %% users, rights etc...
      {publish_settings, [
        {servers, [[
          %{dns_name, name5},
          {host, '{{ rmq_http[0].host }}'},
          {port, {{ rmq_http[0].port }}},
          {username, <<"{{ rmq_http[0].user }}">>},
          {password, <<"{{ rmq_http[0].pass }}">>},
          {vhost, <<"{{ rmq_vhost }}">>}
        ]]},
        {min_size, {{ pub_settings_pool_min }}},
        {max_size, {{ pub_settings_pool_max }}},
        {start_size, {{ pub_settings_pool_start }}}
      ]},

      %% consumer for settings
      %% this messages inform app-s about changes in a structure of processes,
      %% users, rights etc...
      {consumer_settings, [
        {servers, [[
          %{dns_name, name5},
          {host, '{{ rmq_http[0].host }}'},
          {port, {{ rmq_http[0].port }}},
          {username, <<"{{ rmq_http[0].user }}">>},
          {password, <<"{{ rmq_http[0].pass }}">>},
          {vhost, <<"{{ rmq_vhost }}">>}
        ]]},
        {connections_per_queue, {{ cons_settings_connections_per_queue }}},
        {channels_per_connection, {{ cons_settings_channels_per_connection }}},
        {messages_prefetch_size_per_channel, {{ cons_settings_msg_prefetch_size }}}
      ]},

      %% cluster communications
      %% each node have to be able to connect with another nodes using interconnect.interface and interconnect.listen.port options
      {interconnect, [
          {name, workers},
          {id, <<"worker-{{ worker_id | default(ansible_default_ipv4.address) }}">>},    %% should be the same as "worker_id"
          {interface, "eth0"}, %% IP from this interface will be used for sending the first packet in the beginning of communication
          {password, <<"Igu1poh2eimeChahca">>}, %% should be the same on each node in this cluster that uses the same interconnect.name
          {listen, [
            {port, {{ worker_interconnect_port }}},   %% port for internode communication in cluster
            {transport, tcp}, %% allowed values "tcp | ssl (need cert file)"
            %{opts, [{certfile, "/ebsmnt/conf/cp-self-signed.pem"}]}, %% uncomment this option to enable TLS encryption
            {max_connections, 1024} %% max connections from all nodes to this one -> sender.max_size * nodes count in this cluster
          ]},
          %% threads count between workers in the cluster
          {sender, [
            {start_size, 1},
            {min_size, 1},
            {max_size, 100}
          ]}
      ]},

      %% produce messages for all "to_worker" queues
      %% from requests that are coming through API
      %% user's requests on API, requests from user's interface
      {publish_to_worker_request, [
          {servers, [
{% for item in rmq_core %}
            { {{ item.shards }} , [[
              %{dns_name, {{ item.dns_cache_name }}}
              {host, '{{ item.host }}'}
            ]]
{% if rmq_core.index(item) == rmq_core|length - 1 %}
          }
{% else %}
          },
{% endif %}
{% endfor %}
          ]},
          {port, {{ rmq_port }}},
          {username, <<"{{ rmq_user }}">>},
          {password, <<"{{ rmq_user_pass }}">>},
          {vhost, <<"{{ rmq_vhost }}">>},
          {min_size, {{ pub_to_worker_pool_min  | default(5)}}},
          {max_size, {{ pub_to_worker_pool_max  | default(5) }}},
          {start_size, {{ pub_to_worker_pool_start  | default(5) }}}
      ]},

      %% consumer for to_worker
      {consumer_shard_response, [
        {servers, [[
          %{dns_name, main},
          {host, '{{ rmq_host }}'},
          {port, {{ rmq_port }}},
          {username, <<"{{ rmq_user }}">>},
          {password, <<"{{ rmq_user_pass }}">>},
          {vhost, <<"{{ rmq_vhost }}">>}
        ]]},
        {connections_per_queue, {{ cons_shard_connections_per_queue }}},
        {channels_per_connection, {{ cons_shard_channels_per_connection }}},
        {messages_prefetch_size_per_channel, {{ cons_shard_msg_prefetch_size }}}
      ]},

      %% publish git_call request
      {publish_git_call_request, [
        {servers, [[
{#{% if rmq_use_dsn is defined and rmq_use_dsn %}#}
{#          {dns_name, {{ rmq_http[0].dns_cache_name }}},#}
{#          %%{host, '{{ rmq_http[0].host }}'},#}
{#{% else %}#}
{#          %%{dns_name, {{ rmq_http[0].dns_cache_name }}},#}
{#          {host, '{{ rmq_http[0].host }}'},#}
{#{% endif %}#}
          {host, "{{ rmq[0].host }}"},
          {port, {{ rmq[0].port }}},
          {username, <<"{{ rmq[0].user }}">>},
          {password, <<"{{ rmq[0].pass }}">>},
          {vhost, <<"{{ rmq_vhost }}">>}
        ]]},
        {build_dep_queues_count, 1},
        {remove_dep_queues_count, 1},
        {get_script_queues_count, 1},
        {remove_script_queues_count, 1},
        {validate_script_queues_count, 1},
        {compile_script_queues_count, 1},
        {run_script_queues_count, 10},
        {min_size, {{ pub_git_call_pool_min }}},
        {max_size, {{ pub_git_call_pool_max }}},
        {start_size, {{ pub_git_call_pool_start }}}
      ]},

      %% consumer git_call response
      {consumer_git_call_response, [
        {servers, [[
{#{% if rmq_use_dsn is defined and rmq_use_dsn %}#}
{#          {dns_name, {{ rmq_http[0].dns_cache_name }}},#}
{#          %%{host, '{{ rmq_http[0].host }}'},#}
{#{% else %}#}
{#          %%{dns_name, {{ rmq_http[0].dns_cache_name }}},#}
{#          {host, '{{ rmq_http[0].host }}'},#}
{#{% endif %}#}
{#          {port, {{ rmq_http[0].port }}},#}
{#          {username, <<"{{ rmq_http[0].user }}">>},#}
{#          {password, <<"{{ rmq_http[0].pass }}">>},#}
        {host, "{{ rmq[0].host }}"},
        {port, {{ rmq[0].port }}},
        {username, <<"{{ rmq[0].user }}">>},
        {password, <<"{{ rmq[0].pass }}">>},
          {vhost, <<"{{ rmq_vhost }}">>}
        ]]},
        %%{queues_count, 10},
        {connections_per_queue, 1},
        {channels_per_connection, 1},
        {messages_prefetch_size_per_channel, 1}
      ]}
    ]
  },

  %% billing setting
{% if capi.billing is defined %}
  {billing, [
    {conv_id, {{ capi.billing }}},
    {interval, 10}
  ]},
{% else %}
  {billing, []},
{% endif %}

  {lager, [
      %% What handlers to install with what arguments (wrapped by middleman)

      {log_root, "{{ top_dir }}/erlang/{{ item }}/log"},

      {handlers, [
        {lager_console_backend, info },
        {lager_file_backend, [{file, "error.log"}, {level, error}, {size, 734003200}, {date, "$D0"}, {count, 5}]},
        {lager_file_backend, [{file, "console.log"}, {level, info}, {size, 734003200}, {date, "$D0"}, {count, 5}]}
      ]},

      %% What colors to use with what log levels
      {colored, true},
      {colors, [
        {debug,     "\e[0;38m" },
        {info,      "\e[1;37m" },
        {notice,    "\e[1;36m" },
        {warning,   "\e[1;33m" },
        {error,     "\e[1;31m" },
        {critical,  "\e[1;35m" },
        {alert,     "\e[1;44m" },
        {emergency, "\e[1;41m" }

      ]},

      %% Whether to write a crash log, and where. Undefined means no crash logger.
      {crash_log, "crash.log"},
      %% Maximum size in bytes of events in the crash log - defaults to 65536
      {crash_log_msg_size, 65536},
      %% Maximum size of the crash log in bytes, before its rotated, set
      %% to 0 to disable rotation - default is 0
      {crash_log_size, 734003200},
      %% What time to rotate the crash log - default is no time
      %% rotation. See the README for a description of this format.
      {crash_log_date, "$D0"},
      %% Number of rotated crash logs to keep, 0 means keep only the
      %% current one - default is 0
      {crash_log_count, 5},
      %% Whether to redirect error_logger messages into lager - defaults to true
      {error_logger_redirect, true},
      %% How many messages per second to allow from error_logger before we start dropping them
      {error_logger_hwm, 50},
      %% How big the gen_event mailbox can get before it is switched into sync mode
      {async_threshold, 20},
      %% Switch back to async mode, when gen_event mailbox size decrease from 'async_threshold'
      %% to async_threshold - async_threshold_window
      {async_threshold_window, 5}
    ]
  },

{% if hc_server_disabled == "false" %}
  %% Client for server healthcheck
  {hcheck_sender, [
    {host, <<"{{ hc_server_host }}">>}, %% host of the remote healthcheck server
    {port, {{ hc_server_port }}}, %% port of the remote healthcheck server
    {node_name, <<"worker-{{ worker.id | default(ansible_default_ipv4.address) }}">> }, %% different for each node
    {node_type, <<"worker">> }, %% api | worker | multipart | http_worker | usercode | deepmemo ...
    {disabled, {{ hc_server_disabled }}}, %% true by default
    {send_interval_sec, {{ hc_server_send_interval_sec | default(30) }}}, %% by default 10 sec
    {send_system_counters, true} %% memory processes etc, false by default
  ]},
{% else %}
  {hcheck_sender, [
    {disabled, {{ hc_server_disabled }}} %% true by default
  ]},
{% endif %}

  {erlprometheus, [
    {port, {{ worker_prometheus_port }}}
  ]},

  {sasl, [{sasl_error_logger, false}]}
].
