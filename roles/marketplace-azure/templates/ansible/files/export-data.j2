{% set acc_id = instance_doc.stdout|from_json %}
#!/bin/bash

{% include 'files/header.j2' %}


PGPORT=5433
BACKET="corezoid-backups-{{ acc_id.accountId }}"
BACKUPDIR="/ebsmnt/backups"
DATE="$(date +%Y%m%d-%H%M%S)"

PGUSER="internal_user"
SHARDS="$(echo cp{0..1})"
SHARDS_TABLES="charts nodes stream_counters register_stream_counters user_group_privilegies"

REDIS_DUMP="/ebsmnt/redis/dump.rdb"

if [ -d "${BACKUPDIR}" ];
	then
		cd ${BACKUPDIR}
	else
		mkdir -p ${BACKUPDIR} || exit 1
fi

check_access_to_s3() {
	touch /tmp/check_access.txt
	aws s3 mv /tmp/check_access.txt s3://${BACKET}/check_access.txt 1>/dev/null 2>/dev/null
	[ "$?" == 0 ] || { echo "s3://${BACKET} is not accessible for write."; exit 1; }
}

dump_tables() {
	DBNAME="${1}"
	DBDIR="${BACKUPDIR}/${DATE}/${DBNAME}"

	[ -d "${DBDIR}" ] || { mkdir -p "${DBDIR}"; } 

	# if $2 not defined then dump all tables in database
	if [ "$2" ];
		then
			TABLES="${2}"
		else
			TABLES="$(psql --port ${PGPORT} -U ${PGUSER} ${DBNAME} -Atc "select relname from pg_stat_user_tables where schemaname = 'public' ORDER BY relname" 2>/dev/null)"
	fi

	[ ! -z "${TABLES}" ] || { echo "Tables not found. Is PostgreSQL running?"; exit 1; }

	for TABLE in ${TABLES}
	do
		pg_dump -p ${PGPORT} -U ${PGUSER} -c -t ${TABLE} ${DBNAME} -f ${DBDIR}/${TABLE}.sql || exit 1
		#gzip ${BACKUPDIR}/${DATE}/${TABLE}.txt || exit 1
		#aws s3 mv ${BACKUPDIR}/${DATE}/${TABLE}.txt.gz s3://${BACKET}/${DATE}/${TABLE}.txt.gz 1>/dev/null 2>/dev/null || exit 1
	done
}

redis_dump(){
	service redis stop
	sleep 1
	[ -f "${REDIS_DUMP}" ] || { echo "${REDIS_DUMP} does not exist."; }
	cp ${REDIS_DUMP} ${BACKUPDIR}/${DATE}/ || exit 1
	service redis start
}

upload_to_s3(){
	# create archive from all tables
	tar czf ${BACKUPDIR}/corezoid-db-${DATE}.tar.gz -C ${BACKUPDIR}/${DATE} . || { echo "The archiving process hasn't been completed."; exit 1; } 
	MD5_DUMP="$(md5sum ${BACKUPDIR}/corezoid-db-${DATE}.tar.gz | awk '{ print $1 }')"

	aws s3 mv ${BACKUPDIR}/corezoid-db-${DATE}.tar.gz s3://${BACKET}/${DATE}/corezoid-db-${DATE}.tar.gz 1>/dev/null 2>/dev/null
	[ "$?" == 0 ] || { echo "Archive wasn't uploaded correctly. Please check your s3 backet."; exit 1; }

	# md5 check
	aws s3api head-object --if-match "${MD5_DUMP}" --bucket ${BACKET} --key ${DATE}/corezoid-db-${DATE}.tar.gz 1>/dev/null 2>>/dev/null
	[ "$?" == 0 ] || { echo "Archive wasn't uploaded correctly. Please check your s3 backet."; exit 1; }
	rm -fR ${BACKUPDIR}/${DATE}

}

echo "$(date +%Y%m%d\ %H:%M:%S) :: Backup process started."
check_access_to_s3

dump_tables "conveyor"
dump_tables "cce"

for DB in ${SHARDS}
do
	dump_tables "$DB" "$SHARDS_TABLES"
done

redis_dump

upload_to_s3 
echo "$(date +%Y%m%d\ %H:%M:%S) :: Finished."
